{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4 Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# data paths\n",
    "data_X_path = \"cda_lab4_data/X.npy\"\n",
    "data_y_path = \"cda_lab4_data/y.npy\"\n",
    "feature_names_path = \"cda_lab4_data/feature_names.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X dimensions:  (30396, 22761)\n",
      "y dimensions:  (30396,)\n",
      "Feature names:       0                               1\n",
      "0  NaN                    feature_name\n",
      "1  0.0        kernel32.dll:SetFileTime\n",
      "2  1.0    kernel32.dll:CompareFileTime\n",
      "3  2.0        kernel32.dll:SearchPathW\n",
      "4  3.0  kernel32.dll:GetShortPathNameW\n"
     ]
    }
   ],
   "source": [
    "# load dataset\n",
    "X = np.load(data_X_path)\n",
    "print(\"X dimensions: \", X.shape)\n",
    "y = np.load(data_y_path)\n",
    "print(\"y dimensions: \", y.shape)\n",
    "\n",
    "# load feature names\n",
    "feature_names = pd.read_csv(feature_names_path, header=None)\n",
    "print(\"Feature names: \", feature_names.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  24316\n",
      "Train malwares:  12168\n",
      "\n",
      "Test size:  6080\n",
      "Test malwares:  3042\n"
     ]
    }
   ],
   "source": [
    "# split data into train and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print(\"Train size: \", len(y_train))\n",
    "print(\"Train malwares: \", np.sum(y_train == 1))\n",
    "print()\n",
    "print(\"Test size: \", len(y_test))\n",
    "print(\"Test malwares: \", np.sum(y_test == 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network model\n",
    "class SimpleModel(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(SimpleModel, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, targets):\n",
    "    # Calculate accuracy\n",
    "    _, predicted = torch.max(outputs, 1)\n",
    "    total = targets.size(0)\n",
    "    correct = (predicted == targets).sum().item()\n",
    "    return total, correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train step function\n",
    "def train_step(model, data_loader, loss_fn, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for inputs, targets in data_loader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += targets.size(0)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate function\n",
    "def evaluate(model, data_loader, loss_fn, device='cpu'):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            # Calculate accuracy\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    accuracy = correct / total\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train loop function\n",
    "def train_loop(model, train_loader, test_loader, loss_fn, optimizer, epochs, device='cpu'):\n",
    "    for epoch in range(epochs):\n",
    "        train_loss, train_accuracy = train_step(model, train_loader, loss_fn, optimizer, device)\n",
    "        test_loss, test_accuracy = evaluate(model, test_loader, loss_fn, device)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "        print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "        print('-----------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "input_size = X.shape[1]  # Number of features\n",
    "num_classes = 2  # Number of classes\n",
    "model = SimpleModel(input_size, num_classes).to('cpu')\n",
    "batch_size = 1000\n",
    "\n",
    "# Cross entropy loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# Convert to PyTorch datasets\n",
    "train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float), torch.tensor(y_train, dtype=torch.long))\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# For simplicity, using the same dataset for testing\n",
    "test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float), torch.tensor(y_test, dtype=torch.long))\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2:\n",
      "Train Loss: 0.1719, Train Accuracy: 0.9207\n",
      "Test Loss: 0.1657, Test Accuracy: 0.9215\n",
      "-----------------------------------\n",
      "Epoch 2/2:\n",
      "Train Loss: 0.1707, Train Accuracy: 0.9217\n",
      "Test Loss: 0.1744, Test Accuracy: 0.9209\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "train_loop(model, train_loader, test_loader, loss_fn, optimizer, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grams_topk_variant(b, model, loss_fn, target_labels, k_init=8, device='cpu'):\n",
    "    # Convert numpy arrays to torch tensors\n",
    "    b = torch.tensor(b, dtype=torch.float32, requires_grad=True).to(device)\n",
    "    orig_x = b.clone().detach()\n",
    "    best_x = b.clone().detach()\n",
    "    target_labels = torch.tensor(target_labels, dtype=torch.long).to(device)\n",
    "    k = k_init\n",
    "    \n",
    "    def compute_loss(x):\n",
    "        model_output = model(x)\n",
    "        return loss_fn(model_output, target_labels)\n",
    "    \n",
    "    while k > 0.5:\n",
    "        print(\"k: \", k)\n",
    "        # Compute loss\n",
    "        loss = compute_loss(b)\n",
    "        print('loss: ', loss)\n",
    "        \n",
    "        # Zero the gradients\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Compute gradients\n",
    "        loss.backward()\n",
    "        grad = b.grad.data\n",
    "        print('grad: ', grad)\n",
    "        \n",
    "        # Compute the sign of the gradient\n",
    "        sign = grad.sign()\n",
    "        print('sign: ', sign)\n",
    "        \n",
    "        # Adjust the gradient\n",
    "        adjusted_grad = torch.abs(grad - orig_x * grad)\n",
    "        \n",
    "        # Get the top-k elements\n",
    "        topk_indices = torch.topk(adjusted_grad, int(k)).indices\n",
    "        \n",
    "        # Update x with top-k elements and their signs\n",
    "        x_new = b.clone().detach()\n",
    "        x_new[topk_indices] = x_new[topk_indices] + sign[topk_indices]\n",
    "        \n",
    "        # Compute new loss\n",
    "        new_loss = compute_loss(x_new)\n",
    "        print('new_loss: ', new_loss)\n",
    "        \n",
    "        # Compute loss for the best observed x\n",
    "        best_loss = compute_loss(best_x)\n",
    "        print('best_loss: ', best_loss)\n",
    "        \n",
    "        # Update best_x if new loss is better\n",
    "        if new_loss.item() > best_loss.item():\n",
    "            best_x[topk_indices] = x_new[topk_indices]\n",
    "            b = torch.tensor(x_new, requires_grad=True).to(device)\n",
    "            k *= 2\n",
    "        else:\n",
    "            k /= 2\n",
    "    \n",
    "    return best_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1744221800139972, 0.9208881578947369)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, test_loader, loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.1758149480819702, 0.9217798980095411)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(model, train_loader, loss_fn=loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b: torch.Size([22761])\n",
      "k:  8\n",
      "loss:  tensor(0.0032, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 1.0346e-04, -2.0407e-04,  5.9553e-04,  ..., -7.2176e-06,\n",
      "        -5.4561e-06, -1.8637e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "new_loss:  tensor(4.6735, grad_fn=<NllLossBackward0>)\n",
      "best_loss:  tensor(0.0032, grad_fn=<NllLossBackward0>)\n",
      "k:  16\n",
      "loss:  tensor(4.6735, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 0.0319, -0.0630,  0.1838,  ..., -0.0022, -0.0017, -0.0058])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "new_loss:  tensor(23.1323, grad_fn=<NllLossBackward0>)\n",
      "best_loss:  tensor(4.6735, grad_fn=<NllLossBackward0>)\n",
      "k:  32\n",
      "loss:  tensor(23.1323, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 0.0322, -0.0636,  0.1856,  ..., -0.0022, -0.0017, -0.0058])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "new_loss:  tensor(56.5191, grad_fn=<NllLossBackward0>)\n",
      "best_loss:  tensor(23.1323, grad_fn=<NllLossBackward0>)\n",
      "k:  64\n",
      "loss:  tensor(56.5191, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 0.0322, -0.0636,  0.1856,  ..., -0.0022, -0.0017, -0.0058])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "new_loss:  tensor(116.8615, grad_fn=<NllLossBackward0>)\n",
      "best_loss:  tensor(56.5191, grad_fn=<NllLossBackward0>)\n",
      "k:  128\n",
      "loss:  tensor(116.8615, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 0.0322, -0.0636,  0.1856,  ..., -0.0022, -0.0017, -0.0058])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "new_loss:  tensor(225.6945, grad_fn=<NllLossBackward0>)\n",
      "best_loss:  tensor(116.8615, grad_fn=<NllLossBackward0>)\n",
      "k:  256\n",
      "loss:  tensor(225.6945, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 0.0322, -0.0636,  0.1856,  ..., -0.0022, -0.0017, -0.0058])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "new_loss:  tensor(417.9811, grad_fn=<NllLossBackward0>)\n",
      "best_loss:  tensor(225.6945, grad_fn=<NllLossBackward0>)\n",
      "k:  512\n",
      "loss:  tensor(417.9811, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 0.0322, -0.0636,  0.1856,  ..., -0.0022, -0.0017, -0.0058])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "new_loss:  tensor(750.6679, grad_fn=<NllLossBackward0>)\n",
      "best_loss:  tensor(417.9811, grad_fn=<NllLossBackward0>)\n",
      "k:  1024\n",
      "loss:  tensor(750.6679, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 0.0322, -0.0636,  0.1856,  ..., -0.0022, -0.0017, -0.0058])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "new_loss:  tensor(1306.7273, grad_fn=<NllLossBackward0>)\n",
      "best_loss:  tensor(750.6679, grad_fn=<NllLossBackward0>)\n",
      "k:  2048\n",
      "loss:  tensor(1306.7273, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 0.0322, -0.0636,  0.1856,  ..., -0.0022, -0.0017, -0.0058])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "new_loss:  tensor(2188.8782, grad_fn=<NllLossBackward0>)\n",
      "best_loss:  tensor(1306.7273, grad_fn=<NllLossBackward0>)\n",
      "k:  4096\n",
      "loss:  tensor(2188.8782, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 0.0322, -0.0636,  0.1856,  ..., -0.0022, -0.0017, -0.0058])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "new_loss:  tensor(3465.0913, grad_fn=<NllLossBackward0>)\n",
      "best_loss:  tensor(2188.8782, grad_fn=<NllLossBackward0>)\n",
      "k:  8192\n",
      "loss:  tensor(3465.0913, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 0.0322, -0.0636,  0.1856,  ..., -0.0022, -0.0017, -0.0058])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "new_loss:  tensor(5212.2002, grad_fn=<NllLossBackward0>)\n",
      "best_loss:  tensor(3465.0913, grad_fn=<NllLossBackward0>)\n",
      "k:  16384\n",
      "loss:  tensor(5212.2002, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 0.0322, -0.0636,  0.1856,  ..., -0.0022, -0.0017, -0.0058])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "new_loss:  tensor(7232.9429, grad_fn=<NllLossBackward0>)\n",
      "best_loss:  tensor(5212.2002, grad_fn=<NllLossBackward0>)\n",
      "k:  32768\n",
      "loss:  tensor(7232.9429, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 0.0322, -0.0636,  0.1856,  ..., -0.0022, -0.0017, -0.0058])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gn/8cmn0qdj1mx7g0mxnt5q722h0000gp/T/ipykernel_73886/2983822074.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b = torch.tensor(b, dtype=torch.float32, requires_grad=True).to(device)\n",
      "/var/folders/gn/8cmn0qdj1mx7g0mxnt5q722h0000gp/T/ipykernel_73886/2983822074.py:52: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  b = torch.tensor(x_new, requires_grad=True).to(device)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "selected index k out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb:\u001b[39m\u001b[38;5;124m'\u001b[39m, b\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m target_labels \u001b[38;5;241m=\u001b[39m y_train[y_train \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m----> 6\u001b[0m best_x \u001b[38;5;241m=\u001b[39m \u001b[43mgrams_topk_variant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moriginal x:\u001b[39m\u001b[38;5;124m'\u001b[39m, model(b))\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest x:\u001b[39m\u001b[38;5;124m\"\u001b[39m, model(best_x))\n",
      "Cell \u001b[0;32mIn[75], line 35\u001b[0m, in \u001b[0;36mgrams_topk_variant\u001b[0;34m(b, model, loss_fn, target_labels, k_init, device)\u001b[0m\n\u001b[1;32m     32\u001b[0m adjusted_grad \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mabs(grad \u001b[38;5;241m-\u001b[39m orig_x \u001b[38;5;241m*\u001b[39m grad)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Get the top-k elements\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m topk_indices \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[43madjusted_grad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mindices\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Update x with top-k elements and their signs\u001b[39;00m\n\u001b[1;32m     38\u001b[0m x_new \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: selected index k out of range"
     ]
    }
   ],
   "source": [
    "b = X_train[y_train == 1][0]\n",
    "b = torch.tensor(b, dtype=torch.float32).to('cpu')\n",
    "print('b:', b.shape)\n",
    "target_labels = y_train[y_train == 1][0]\n",
    "\n",
    "best_x = grams_topk_variant(b, model, loss_fn, target_labels)\n",
    "print('original x:', model(b))\n",
    "print(\"Best x:\", model(best_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "k:  8\n",
      "loss:  tensor(0.0032, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 1.0346e-04, -2.0407e-04,  5.9553e-04,  ..., -7.2176e-06,\n",
      "        -5.4561e-06, -1.8637e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  4.0\n",
      "loss:  tensor(0.0032, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 2.0692e-04, -4.0813e-04,  1.1911e-03,  ..., -1.4435e-05,\n",
      "        -1.0912e-05, -3.7275e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  2.0\n",
      "loss:  tensor(0.0032, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 3.1039e-04, -6.1220e-04,  1.7866e-03,  ..., -2.1653e-05,\n",
      "        -1.6368e-05, -5.5912e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  1.0\n",
      "loss:  tensor(0.0032, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 4.1385e-04, -8.1627e-04,  2.3821e-03,  ..., -2.8871e-05,\n",
      "        -2.1824e-05, -7.4549e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  8\n",
      "loss:  tensor(0.0004, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 1.2706e-05, -2.5062e-05,  7.3137e-05,  ..., -8.8632e-07,\n",
      "        -6.7019e-07, -2.2887e-06])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  4.0\n",
      "loss:  tensor(0.0004, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 2.5412e-05, -5.0123e-05,  1.4627e-04,  ..., -1.7726e-06,\n",
      "        -1.3404e-06, -4.5775e-06])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  2.0\n",
      "loss:  tensor(0.0004, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 3.8118e-05, -7.5185e-05,  2.1941e-04,  ..., -2.6590e-06,\n",
      "        -2.0106e-06, -6.8662e-06])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  1.0\n",
      "loss:  tensor(0.0004, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 5.0825e-05, -1.0025e-04,  2.9255e-04,  ..., -3.5453e-06,\n",
      "        -2.6808e-06, -9.1549e-06])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  8\n",
      "loss:  tensor(0.0062, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 1.9986e-04, -3.9420e-04,  1.1504e-03,  ..., -1.3943e-05,\n",
      "        -1.0539e-05, -3.6002e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  4.0\n",
      "loss:  tensor(0.0062, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 3.9972e-04, -7.8840e-04,  2.3008e-03,  ..., -2.7885e-05,\n",
      "        -2.1079e-05, -7.2004e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  2.0\n",
      "loss:  tensor(0.0062, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 5.9958e-04, -1.1826e-03,  3.4512e-03,  ..., -4.1828e-05,\n",
      "        -3.1618e-05, -1.0801e-04])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  1.0\n",
      "loss:  tensor(0.0062, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 7.9944e-04, -1.5768e-03,  4.6016e-03,  ..., -5.5770e-05,\n",
      "        -4.2158e-05, -1.4401e-04])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  8\n",
      "loss:  tensor(0.0007, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 2.4028e-05, -4.7393e-05,  1.3831e-04,  ..., -1.6763e-06,\n",
      "        -1.2671e-06, -4.3284e-06])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  4.0\n",
      "loss:  tensor(0.0007, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 4.8057e-05, -9.4786e-05,  2.7661e-04,  ..., -3.3526e-06,\n",
      "        -2.5342e-06, -8.6568e-06])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  2.0\n",
      "loss:  tensor(0.0007, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 7.2085e-05, -1.4218e-04,  4.1492e-04,  ..., -5.0288e-06,\n",
      "        -3.8013e-06, -1.2985e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  1.0\n",
      "loss:  tensor(0.0007, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 9.6113e-05, -1.8957e-04,  5.5323e-04,  ..., -6.7051e-06,\n",
      "        -5.0684e-06, -1.7314e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  8\n",
      "loss:  tensor(0.0062, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 1.9986e-04, -3.9420e-04,  1.1504e-03,  ..., -1.3943e-05,\n",
      "        -1.0539e-05, -3.6002e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  4.0\n",
      "loss:  tensor(0.0062, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 3.9972e-04, -7.8840e-04,  2.3008e-03,  ..., -2.7885e-05,\n",
      "        -2.1079e-05, -7.2004e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  2.0\n",
      "loss:  tensor(0.0062, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 5.9958e-04, -1.1826e-03,  3.4512e-03,  ..., -4.1828e-05,\n",
      "        -3.1618e-05, -1.0801e-04])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  1.0\n",
      "loss:  tensor(0.0062, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 7.9944e-04, -1.5768e-03,  4.6016e-03,  ..., -5.5770e-05,\n",
      "        -4.2158e-05, -1.4401e-04])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  8\n",
      "loss:  tensor(0.0032, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 1.0346e-04, -2.0407e-04,  5.9553e-04,  ..., -7.2176e-06,\n",
      "        -5.4561e-06, -1.8637e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  4.0\n",
      "loss:  tensor(0.0032, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 2.0692e-04, -4.0813e-04,  1.1911e-03,  ..., -1.4435e-05,\n",
      "        -1.0912e-05, -3.7275e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  2.0\n",
      "loss:  tensor(0.0032, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 3.1039e-04, -6.1220e-04,  1.7866e-03,  ..., -2.1653e-05,\n",
      "        -1.6368e-05, -5.5912e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  1.0\n",
      "loss:  tensor(0.0032, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 4.1385e-04, -8.1627e-04,  2.3821e-03,  ..., -2.8871e-05,\n",
      "        -2.1824e-05, -7.4549e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  8\n",
      "loss:  tensor(0.0068, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 2.1789e-04, -4.2977e-04,  1.2542e-03,  ..., -1.5201e-05,\n",
      "        -1.1491e-05, -3.9250e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  4.0\n",
      "loss:  tensor(0.0068, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 4.3578e-04, -8.5954e-04,  2.5084e-03,  ..., -3.0401e-05,\n",
      "        -2.2981e-05, -7.8501e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  2.0\n",
      "loss:  tensor(0.0068, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 6.5368e-04, -1.2893e-03,  3.7626e-03,  ..., -4.5602e-05,\n",
      "        -3.4472e-05, -1.1775e-04])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  1.0\n",
      "loss:  tensor(0.0068, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 8.7157e-04, -1.7191e-03,  5.0168e-03,  ..., -6.0802e-05,\n",
      "        -4.5962e-05, -1.5700e-04])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  8\n",
      "loss:  tensor(0.0015, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 4.7539e-05, -9.3766e-05,  2.7364e-04,  ..., -3.3164e-06,\n",
      "        -2.5070e-06, -8.5635e-06])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  4.0\n",
      "loss:  tensor(0.0015, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 9.5078e-05, -1.8753e-04,  5.4727e-04,  ..., -6.6328e-06,\n",
      "        -5.0140e-06, -1.7127e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  2.0\n",
      "loss:  tensor(0.0015, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 1.4262e-04, -2.8130e-04,  8.2091e-04,  ..., -9.9492e-06,\n",
      "        -7.5211e-06, -2.5691e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  1.0\n",
      "loss:  tensor(0.0015, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 1.9016e-04, -3.7506e-04,  1.0945e-03,  ..., -1.3266e-05,\n",
      "        -1.0028e-05, -3.4254e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  8\n",
      "loss:  tensor(0.0028, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 9.0775e-05, -1.7904e-04,  5.2250e-04,  ..., -6.3324e-06,\n",
      "        -4.7873e-06, -1.6352e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  4.0\n",
      "loss:  tensor(0.0028, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 1.8155e-04, -3.5809e-04,  1.0450e-03,  ..., -1.2665e-05,\n",
      "        -9.5747e-06, -3.2703e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  2.0\n",
      "loss:  tensor(0.0028, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 2.7233e-04, -5.3713e-04,  1.5675e-03,  ..., -1.8997e-05,\n",
      "        -1.4362e-05, -4.9055e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  1.0\n",
      "loss:  tensor(0.0028, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 3.6310e-04, -7.1618e-04,  2.0900e-03,  ..., -2.5330e-05,\n",
      "        -1.9149e-05, -6.5407e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  8\n",
      "loss:  tensor(0.0016, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 4.9930e-05, -9.8482e-05,  2.8740e-04,  ..., -3.4831e-06,\n",
      "        -2.6334e-06, -8.9941e-06])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  4.0\n",
      "loss:  tensor(0.0016, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 9.9861e-05, -1.9696e-04,  5.7480e-04,  ..., -6.9661e-06,\n",
      "        -5.2667e-06, -1.7988e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  2.0\n",
      "loss:  tensor(0.0016, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 1.4979e-04, -2.9545e-04,  8.6220e-04,  ..., -1.0449e-05,\n",
      "        -7.9001e-06, -2.6982e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n",
      "k:  1.0\n",
      "loss:  tensor(0.0016, grad_fn=<NllLossBackward0>)\n",
      "grad:  tensor([ 1.9972e-04, -3.9393e-04,  1.1496e-03,  ..., -1.3932e-05,\n",
      "        -1.0533e-05, -3.5976e-05])\n",
      "sign:  tensor([ 1., -1.,  1.,  ..., -1., -1., -1.])\n"
     ]
    }
   ],
   "source": [
    "best_xs = []\n",
    "for i in range(10):\n",
    "    b = X_train[y_train == 1][i]\n",
    "    target_labels = y_train[y_train == 1][i]\n",
    "    best_x = grams_topk_variant(b, model, loss_fn, target_labels)\n",
    "    best_xs.append(best_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_labels = y_train[y_train == 1][:10]\n",
    "evaluate_model(model, best_xs, target_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_xs = X_train[y_train == 1][:10]\n",
    "target_labels = y_train[y_train == 1][:10]\n",
    "evaluate_model(model, best_xs, target_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
